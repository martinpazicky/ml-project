{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "237b78d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv('data/processed/BankChurners_after_preprocessing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6251988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 5\n",
    "TARGET_COL_NAME = 'Attrition_Flag'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "5b0a97fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target column\n",
    "df[TARGET_COL_NAME].replace({'Existing Customer': 1, 'Attrited Customer': 0}, inplace=True)\n",
    "\n",
    "# Encode boolean columns using astype(int) method\n",
    "bool_columns = ['Gender_F', 'Gender_M', 'Marital_Status_Divorced', 'Marital_Status_Married', 'Marital_Status_Single']\n",
    "df[bool_columns] = df[bool_columns].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55be3443",
   "metadata": {},
   "source": [
    "Split the data to training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e28dba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df[TARGET_COL_NAME]\n",
    "df_train = drop_col(df, 'id')\n",
    "df_train = drop_col(df_train, TARGET_COL_NAME)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train, target, test_size=0.25, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aca7fb",
   "metadata": {},
   "source": [
    "### Baseline: Logistic Regression\n",
    "\n",
    "We'll use logistic regression as a baseline, just like in the laboratories, and utilize the other models to try to enhance the findings provided by the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fa5f869f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8874407582938388\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=1000, tol=0.001)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "76c69a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply oversampling to the training set using SMOTE\n",
    "oversampler = SMOTE(random_state=RANDOM_STATE)\n",
    "X_train, y_train = oversampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0d09b573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8364928909952607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\romib\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=1000, tol=0.001)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510a9dac",
   "metadata": {},
   "source": [
    "### Multi Layered Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fce0f960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8431863067807768\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import confusion_matrix,\\\n",
    "                classification_report, accuracy_score\n",
    "\n",
    "\n",
    "model_nnet = MLPClassifier(hidden_layer_sizes=[5,5,5],\n",
    "                           alpha=0.001,\n",
    "                           activation='logistic',\n",
    "                           max_iter=800,\n",
    "                           solver='lbfgs',random_state=RANDOM_STATE)\n",
    "model_nnet.fit(X_train,y_train)\n",
    "y_pred = model_nnet.predict(X_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a4c264a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the input features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Apply oversampling to the training set using SMOTE\n",
    "oversampler = SMOTE(random_state=RANDOM_STATE)\n",
    "X_train, y_train = oversampler.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8778e4",
   "metadata": {},
   "source": [
    "Results from MLP strongly depend on the setting of its hyperparameters. Since the training does not take too long, we decided to use Grid search for various hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "52b24fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'alpha': 0.03845401188473625, 'hidden_layer_sizes': [8, 8]}\n",
      "Best F1 Score: 0.947049271900843\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "\n",
    "# Define the parameter grid for random search\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [[2], [2, 2], [4, 4], [5, 5], [8, 8], [2, 2, 2], [8, 8, 8]],\n",
    "    'alpha': uniform(loc=0.001, scale=0.1)\n",
    "}\n",
    "\n",
    "# Create an MLPClassifier instance\n",
    "model_nnet = MLPClassifier(activation='logistic', max_iter=1000, solver='lbfgs', random_state=RANDOM_STATE)\n",
    "\n",
    "# Perform random search with F1 score as the scoring metric, ignore warning that training stopped on max_iter\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "    random_search = RandomizedSearchCV(model_nnet, param_distributions=param_grid, n_iter=10, random_state=42,\n",
    "                                       scoring='f1')\n",
    "    random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = random_search.best_params_\n",
    "best_score = random_search.best_score_\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best F1 Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f03a831d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9647095565271705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\romib\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:549: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "model_nnet = MLPClassifier(hidden_layer_sizes=best_params['hidden_layer_sizes'],\n",
    "                           alpha=best_params['alpha'],\n",
    "                           activation='logistic',\n",
    "                           max_iter=2500,\n",
    "                           solver='lbfgs',random_state=RANDOM_STATE)\n",
    "model_nnet.fit(X_train,y_train)\n",
    "y_pred = model_nnet.predict(X_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_train, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55d6b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07505a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data\n",
    "X = df.drop([TARGET_COL_NAME], axis=1).values\n",
    "y = df[TARGET_COL_NAME].values\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=RANDOM_STATE)\n",
    "\n",
    "# Scale the input features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Apply oversampling to the training set using SMOTE\n",
    "oversampler = SMOTE(random_state=RANDOM_STATE)\n",
    "X_train, y_train = oversampler.fit_resample(X_train, y_train)\n",
    "\n",
    "# Convert the numpy arrays to tensors\n",
    "X_train = torch.Tensor(X_train)\n",
    "y_train = torch.Tensor(y_train).view(-1, 1)\n",
    "X_test = torch.Tensor(X_test)\n",
    "y_test = torch.Tensor(y_test).view(-1, 1)\n",
    "\n",
    "# Define the neural network architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 8)\n",
    "        self.dropout1 = nn.Dropout(0.5)  # Dropout layer with 50% probability\n",
    "        self.fc2 = nn.Linear(8, 4)\n",
    "        self.dropout2 = nn.Dropout(0.5)  # Dropout layer with 50% probability\n",
    "        self.fc3 = nn.Linear(4, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the neural network\n",
    "input_size = X_train.shape[1]\n",
    "net = Net(input_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.0006)\n",
    "\n",
    "# Train the neural network\n",
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "loss_values = []  # To store the loss values\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i in range(0, X_train.size(0), batch_size):\n",
    "        inputs = X_train[i:i+batch_size]\n",
    "        targets = y_train[i:i+batch_size]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Store the loss value\n",
    "    loss_values.append(loss.item())\n",
    "\n",
    "    # Print the loss every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
    "\n",
    "# Plot the loss values\n",
    "plt.plot(loss_values)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the trained model on the test set\n",
    "with torch.no_grad():\n",
    "    net.eval()\n",
    "    y_pred = net(X_test)\n",
    "    y_pred = torch.round(y_pred)\n",
    "\n",
    "    accuracy = (y_pred == y_test).sum().item() / y_test.size(0)\n",
    "    print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e145e071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c11dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4bfd8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
