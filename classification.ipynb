{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "237b78d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility import *\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f57a9ef",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b583799",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/processed/BankChurners_after_preprocessing.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6251988a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 5\n",
    "TARGET_COL_NAME = 'Attrition_Flag'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286e78a0",
   "metadata": {},
   "source": [
    "Main metric will be F1 macro since the dataset is imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6c4ba64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(truth, prediction, index):\n",
    "    acc = metrics.accuracy_score(truth, prediction)\n",
    "    f1 = metrics.f1_score(truth, prediction, average='macro')\n",
    "    prec = metrics.precision_score(truth, prediction, average='macro')\n",
    "    rec = metrics.recall_score(truth, prediction, average='macro')\n",
    "\n",
    "    scores = pd.DataFrame({'F-score': [f1], 'Precision': [prec], 'Accuracy': [acc], 'Recall': [rec]}, index=[index])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c0a272",
   "metadata": {},
   "source": [
    "Encode the category columns as more advanced classification models like Neural Network require that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38b2d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the target column\n",
    "df[TARGET_COL_NAME].replace({'Existing Customer': 1, 'Attrited Customer': 0}, inplace=True)\n",
    "\n",
    "# Encode boolean columns using astype(int) method\n",
    "bool_columns = ['Gender_F', 'Gender_M', 'Marital_Status_Divorced', 'Marital_Status_Married', 'Marital_Status_Single']\n",
    "df[bool_columns] = df[bool_columns].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55be3443",
   "metadata": {},
   "source": [
    "Split the data to training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e28dba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df[TARGET_COL_NAME]\n",
    "df_train = drop_col(df, 'id')\n",
    "df_train = drop_col(df_train, TARGET_COL_NAME)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train, target, test_size=0.25, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aca7fb",
   "metadata": {},
   "source": [
    "### Baseline: Logistic Regression\n",
    "##### Logistic Regression v1: fitting the model (no oversampling, no hyperparameter tunning)\n",
    "\n",
    "In our pursuit of finding an appropriate baseline model for our classification task, we carefully considered various options and ultimately decided to employ Logistic Regression. Logistic Regression is a well-established and widely-used statistical technique that is particularly suited for binary classification problems. Its simplicity and interpretability make it an ideal choice for establishing a benchmark performance level. By fitting a logistic regression model to our labeled training data, we can assess its ability to discriminate between the two classes and establish a baseline accuracy for comparison with more advanced algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa5f869f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F-score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>logistic_regression_train_v1</th>\n",
       "      <td>0.767749</td>\n",
       "      <td>0.825442</td>\n",
       "      <td>0.893878</td>\n",
       "      <td>0.733736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_regression_test_v1</th>\n",
       "      <td>0.770607</td>\n",
       "      <td>0.832473</td>\n",
       "      <td>0.887441</td>\n",
       "      <td>0.735834</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               F-score  Precision  Accuracy    Recall\n",
       "logistic_regression_train_v1  0.767749   0.825442  0.893878  0.733736\n",
       "logistic_regression_test_v1   0.770607   0.832473  0.887441  0.735834"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(max_iter=5000, tol=0.001)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "y_pred = model.predict(X_test)\n",
    "rf_scores_test = evaluate(y_test, y_pred, 'logistic_regression_test_v1')\n",
    "\n",
    "results_df = rf_scores_test\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f04142",
   "metadata": {},
   "source": [
    "##### Logistic Regression v2: fitting the model (with oversampling, scaling and hyperparameter tunning)\n",
    "\n",
    "We will try to improve performance of Logistic Regression with help of feature scaling, handle imbalanced data using SMOTE and simple hyperparameter tuning for following parameters:\n",
    "- C - Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization)\n",
    "- solver - Algorithm to use in the optimization problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b779c88a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F-score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>logistic_regression_train_v1</th>\n",
       "      <td>0.767749</td>\n",
       "      <td>0.825442</td>\n",
       "      <td>0.893878</td>\n",
       "      <td>0.733736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_regression_test_v1</th>\n",
       "      <td>0.770607</td>\n",
       "      <td>0.832473</td>\n",
       "      <td>0.887441</td>\n",
       "      <td>0.735834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_regression_train_v2</th>\n",
       "      <td>0.777363</td>\n",
       "      <td>0.745064</td>\n",
       "      <td>0.854378</td>\n",
       "      <td>0.852135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_regression_test_v2</th>\n",
       "      <td>0.782388</td>\n",
       "      <td>0.753382</td>\n",
       "      <td>0.851896</td>\n",
       "      <td>0.844242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               F-score  Precision  Accuracy    Recall\n",
       "logistic_regression_train_v1  0.767749   0.825442  0.893878  0.733736\n",
       "logistic_regression_test_v1   0.770607   0.832473  0.887441  0.735834\n",
       "logistic_regression_train_v2  0.777363   0.745064  0.854378  0.852135\n",
       "logistic_regression_test_v2   0.782388   0.753382  0.851896  0.844242"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create custom scoring function\n",
    "def custom_scoring(estimator, X, y_true):\n",
    "    y_pred = estimator.predict(X)\n",
    "    return metrics.f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Oversampling - imbalanced data handling\n",
    "smote = SMOTE(random_state=RANDOM_STATE)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# Perform hyperparameter tuning\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1.0, 10.0],\n",
    "    'solver': ['liblinear', 'saga', 'lbfgs', 'newton-cg']\n",
    "}\n",
    "\n",
    "model = LogisticRegression(max_iter=500, tol=0.001)\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring=custom_scoring)\n",
    "grid_search.fit(X_train_balanced, y_train_balanced)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "y_test_pred = best_model.predict(X_test_scaled)\n",
    "rf_scores_test = evaluate(y_test, y_test_pred, 'logistic_regression_test_v2')\n",
    "\n",
    "results_df = pd.concat([results_df,rf_scores_test], axis=0)\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e514e6b",
   "metadata": {},
   "source": [
    "We were able to sligthly improve F-score of Logistic Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f3768a",
   "metadata": {},
   "source": [
    "### Multi Layered Perceptron (MLP)\n",
    "\n",
    "##### MLP v1: fitting the model (with oversampling and scaling)\n",
    "\n",
    "After using Logistic Regression as a baseline for classification, we opted for a Multi Layered Perceptron (MLP) due to its ability to capture complex relationships in the data. MLP's multi-layer structure and non-linear activation functions enable it to model intricate patterns and extract hierarchical representations, making it a suitable choice for tasks involving non-linear data or high-dimensional datasets. By leveraging MLP's capabilities, we aimed to enhance the model's predictive performance and accommodate the complexities present in our classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cafd2ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F-score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>logistic_regression_train_v1</th>\n",
       "      <td>0.767749</td>\n",
       "      <td>0.825442</td>\n",
       "      <td>0.893878</td>\n",
       "      <td>0.733736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_regression_test_v1</th>\n",
       "      <td>0.770607</td>\n",
       "      <td>0.832473</td>\n",
       "      <td>0.887441</td>\n",
       "      <td>0.735834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_regression_train_v2</th>\n",
       "      <td>0.777363</td>\n",
       "      <td>0.745064</td>\n",
       "      <td>0.854378</td>\n",
       "      <td>0.852135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_regression_test_v2</th>\n",
       "      <td>0.782388</td>\n",
       "      <td>0.753382</td>\n",
       "      <td>0.851896</td>\n",
       "      <td>0.844242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_train_v1</th>\n",
       "      <td>0.779465</td>\n",
       "      <td>0.746933</td>\n",
       "      <td>0.855958</td>\n",
       "      <td>0.854097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_test_v1</th>\n",
       "      <td>0.782691</td>\n",
       "      <td>0.753878</td>\n",
       "      <td>0.852686</td>\n",
       "      <td>0.842903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               F-score  Precision  Accuracy    Recall\n",
       "logistic_regression_train_v1  0.767749   0.825442  0.893878  0.733736\n",
       "logistic_regression_test_v1   0.770607   0.832473  0.887441  0.735834\n",
       "logistic_regression_train_v2  0.777363   0.745064  0.854378  0.852135\n",
       "logistic_regression_test_v2   0.782388   0.753382  0.851896  0.844242\n",
       "mlp_train_v1                  0.779465   0.746933  0.855958  0.854097\n",
       "mlp_test_v1                   0.782691   0.753878  0.852686  0.842903"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = df[TARGET_COL_NAME]\n",
    "df_train = drop_col(df, 'id')\n",
    "df_train = drop_col(df_train, TARGET_COL_NAME)\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train, target, test_size=0.25, random_state=RANDOM_STATE)\n",
    "\n",
    "model_nnet = MLPClassifier(hidden_layer_sizes=[1],\n",
    "                           activation='relu',\n",
    "                           max_iter=200,\n",
    "                           solver='adam',\n",
    "                           random_state=42)\n",
    "\n",
    "model_nnet.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "y_pred_test = model_nnet.predict(X_test_scaled)\n",
    "rf_scores_test = evaluate(y_test, y_pred_test, 'mlp_test_v1')\n",
    "\n",
    "results_df = pd.concat([results_df,rf_scores_test], ignore_index=False, sort=False)\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04888516",
   "metadata": {},
   "source": [
    "##### MLP v2: fitting the model (with oversampling, scaling and hyperparameter tunning)\n",
    "\n",
    "Results from MLP strongly depend on the setting of its hyperparameters. Therefore besides scaling (that is also important to apply before training neural networks) and oversampling we will use random search to select best hyperparameters namly:\n",
    "- hidden_layer_sizes - The ith element represents the number of neurons in the ith hidden layer\n",
    "- solver - The solver for weight optimization\n",
    "- alpha - Strength of the L2 regularization term. The L2 regularization term is divided by the sample size when added to the loss.\n",
    "- activation - Activation function for the hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b31bbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'activation': 'relu', 'alpha': 0.01006064345328208, 'hidden_layer_sizes': [30, 30], 'solver': 'adam'}\n",
      "Best F1 Score: 0.9654762342865016\n"
     ]
    }
   ],
   "source": [
    "# Define the parameter grid for random search\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [[2], [2, 2], [4, 4], [5, 5], [8, 8], [30, 30], [4, 4, 4], [20, 20, 20]],\n",
    "    'alpha': uniform(loc=0.001, scale=0.1),\n",
    "    'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "    'activation': ['logistic', 'relu']\n",
    "}\n",
    "\n",
    "model_nnet = MLPClassifier(max_iter=1000, random_state=RANDOM_STATE)\n",
    "\n",
    "# Perform random search with custom F1 score as the scoring metric, ignore warning that training stopped on max_iter\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "    random_search = RandomizedSearchCV(model_nnet, param_distributions=param_grid, n_iter=10, random_state=42,\n",
    "                                       scoring=custom_scoring)\n",
    "    random_search.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Get the best hyperparameters and evaluate the model\n",
    "best_params = random_search.best_params_\n",
    "best_score = random_search.best_score_\n",
    "\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best F1 Score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3399531",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F-score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>logistic_regression_train_v1</th>\n",
       "      <td>0.767749</td>\n",
       "      <td>0.825442</td>\n",
       "      <td>0.893878</td>\n",
       "      <td>0.733736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_regression_test_v1</th>\n",
       "      <td>0.770607</td>\n",
       "      <td>0.832473</td>\n",
       "      <td>0.887441</td>\n",
       "      <td>0.735834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_regression_train_v2</th>\n",
       "      <td>0.777363</td>\n",
       "      <td>0.745064</td>\n",
       "      <td>0.854378</td>\n",
       "      <td>0.852135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_regression_test_v2</th>\n",
       "      <td>0.782388</td>\n",
       "      <td>0.753382</td>\n",
       "      <td>0.851896</td>\n",
       "      <td>0.844242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_train_v1</th>\n",
       "      <td>0.779465</td>\n",
       "      <td>0.746933</td>\n",
       "      <td>0.855958</td>\n",
       "      <td>0.854097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_test_v1</th>\n",
       "      <td>0.782691</td>\n",
       "      <td>0.753878</td>\n",
       "      <td>0.852686</td>\n",
       "      <td>0.842903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_train_v2</th>\n",
       "      <td>0.987022</td>\n",
       "      <td>0.979278</td>\n",
       "      <td>0.993022</td>\n",
       "      <td>0.995178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_test_v2</th>\n",
       "      <td>0.878598</td>\n",
       "      <td>0.869188</td>\n",
       "      <td>0.928910</td>\n",
       "      <td>0.888943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               F-score  Precision  Accuracy    Recall\n",
       "logistic_regression_train_v1  0.767749   0.825442  0.893878  0.733736\n",
       "logistic_regression_test_v1   0.770607   0.832473  0.887441  0.735834\n",
       "logistic_regression_train_v2  0.777363   0.745064  0.854378  0.852135\n",
       "logistic_regression_test_v2   0.782388   0.753382  0.851896  0.844242\n",
       "mlp_train_v1                  0.779465   0.746933  0.855958  0.854097\n",
       "mlp_test_v1                   0.782691   0.753878  0.852686  0.842903\n",
       "mlp_train_v2                  0.987022   0.979278  0.993022  0.995178\n",
       "mlp_test_v2                   0.878598   0.869188  0.928910  0.888943"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all metrics for model with best hyperparameters\n",
    "model_nnet = MLPClassifier(hidden_layer_sizes=best_params['hidden_layer_sizes'],\n",
    "                           alpha=best_params['alpha'],\n",
    "                           activation=best_params['activation'],\n",
    "                           max_iter=2500,\n",
    "                           solver=best_params['solver'],\n",
    "                           random_state=RANDOM_STATE)\n",
    "model_nnet.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "y_pred_test = model_nnet.predict(X_test_scaled)\n",
    "rf_scores_test = evaluate(y_test, y_pred_test, 'mlp_test_v2')\n",
    "\n",
    "results_df = pd.concat([results_df, rf_scores_test], ignore_index=False, sort=False)\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9790a111",
   "metadata": {},
   "source": [
    "### Random forest\n",
    "##### Random forest v1: fitting the model (no oversampling, no hyperparameter tunning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d059920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F-score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>logistic_regression_train_v1</th>\n",
       "      <td>0.767749</td>\n",
       "      <td>0.825442</td>\n",
       "      <td>0.893878</td>\n",
       "      <td>0.733736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_regression_test_v1</th>\n",
       "      <td>0.770607</td>\n",
       "      <td>0.832473</td>\n",
       "      <td>0.887441</td>\n",
       "      <td>0.735834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_regression_train_v2</th>\n",
       "      <td>0.777363</td>\n",
       "      <td>0.745064</td>\n",
       "      <td>0.854378</td>\n",
       "      <td>0.852135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_regression_test_v2</th>\n",
       "      <td>0.782388</td>\n",
       "      <td>0.753382</td>\n",
       "      <td>0.851896</td>\n",
       "      <td>0.844242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_train_v1</th>\n",
       "      <td>0.779465</td>\n",
       "      <td>0.746933</td>\n",
       "      <td>0.855958</td>\n",
       "      <td>0.854097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_test_v1</th>\n",
       "      <td>0.782691</td>\n",
       "      <td>0.753878</td>\n",
       "      <td>0.852686</td>\n",
       "      <td>0.842903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_train_v2</th>\n",
       "      <td>0.987022</td>\n",
       "      <td>0.979278</td>\n",
       "      <td>0.993022</td>\n",
       "      <td>0.995178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_test_v2</th>\n",
       "      <td>0.878598</td>\n",
       "      <td>0.869188</td>\n",
       "      <td>0.928910</td>\n",
       "      <td>0.888943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_train_v1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_test_v1</th>\n",
       "      <td>0.915290</td>\n",
       "      <td>0.935881</td>\n",
       "      <td>0.953791</td>\n",
       "      <td>0.897614</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               F-score  Precision  Accuracy    Recall\n",
       "logistic_regression_train_v1  0.767749   0.825442  0.893878  0.733736\n",
       "logistic_regression_test_v1   0.770607   0.832473  0.887441  0.735834\n",
       "logistic_regression_train_v2  0.777363   0.745064  0.854378  0.852135\n",
       "logistic_regression_test_v2   0.782388   0.753382  0.851896  0.844242\n",
       "mlp_train_v1                  0.779465   0.746933  0.855958  0.854097\n",
       "mlp_test_v1                   0.782691   0.753878  0.852686  0.842903\n",
       "mlp_train_v2                  0.987022   0.979278  0.993022  0.995178\n",
       "mlp_test_v2                   0.878598   0.869188  0.928910  0.888943\n",
       "rf_train_v1                   1.000000   1.000000  1.000000  1.000000\n",
       "rf_test_v1                    0.915290   0.935881  0.953791  0.897614"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "rf_scores_test = evaluate(y_test, y_pred, 'rf_test_v1')\n",
    "\n",
    "results_df = pd.concat([results_df,rf_scores_test], ignore_index=False, sort=False)\n",
    "results_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dce433f",
   "metadata": {},
   "source": [
    "Which features were most important for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "60823252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Total_Trans_Amt</th>\n",
       "      <td>0.188222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total_Trans_Ct</th>\n",
       "      <td>0.158148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total_Ct_Chng_Q4_Q1</th>\n",
       "      <td>0.114927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total_Revolving_Bal</th>\n",
       "      <td>0.112310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total_Relationship_Count</th>\n",
       "      <td>0.061656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total_Amt_Chng_Q4_Q1</th>\n",
       "      <td>0.060624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avg_Utilization_Ratio</th>\n",
       "      <td>0.058185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Credit_Limit</th>\n",
       "      <td>0.034814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Avg_Open_To_Buy</th>\n",
       "      <td>0.033551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Customer_Age</th>\n",
       "      <td>0.032912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Contacts_Count_12_mon</th>\n",
       "      <td>0.028187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Months_on_book</th>\n",
       "      <td>0.026884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Months_Inactive_12_mon</th>\n",
       "      <td>0.023799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dependent_count</th>\n",
       "      <td>0.014102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Education_Level</th>\n",
       "      <td>0.011439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Income_Category</th>\n",
       "      <td>0.009474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender_M</th>\n",
       "      <td>0.007632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marital_Status_Married</th>\n",
       "      <td>0.007224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gender_F</th>\n",
       "      <td>0.006020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marital_Status_Single</th>\n",
       "      <td>0.004761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Card_Category</th>\n",
       "      <td>0.003035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marital_Status_Divorced</th>\n",
       "      <td>0.002094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 0\n",
       "Total_Trans_Amt           0.188222\n",
       "Total_Trans_Ct            0.158148\n",
       "Total_Ct_Chng_Q4_Q1       0.114927\n",
       "Total_Revolving_Bal       0.112310\n",
       "Total_Relationship_Count  0.061656\n",
       "Total_Amt_Chng_Q4_Q1      0.060624\n",
       "Avg_Utilization_Ratio     0.058185\n",
       "Credit_Limit              0.034814\n",
       "Avg_Open_To_Buy           0.033551\n",
       "Customer_Age              0.032912\n",
       "Contacts_Count_12_mon     0.028187\n",
       "Months_on_book            0.026884\n",
       "Months_Inactive_12_mon    0.023799\n",
       "Dependent_count           0.014102\n",
       "Education_Level           0.011439\n",
       "Income_Category           0.009474\n",
       "Gender_M                  0.007632\n",
       "Marital_Status_Married    0.007224\n",
       "Gender_F                  0.006020\n",
       "Marital_Status_Single     0.004761\n",
       "Card_Category             0.003035\n",
       "Marital_Status_Divorced   0.002094"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(clf.feature_importances_, index=df_train.columns).sort_values(by=0, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeca74f9",
   "metadata": {},
   "source": [
    "##### Random forest v2: fitting the model (with oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec240171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampled data length:  12808\n",
      "Attrited customers proportion:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\romib\\AppData\\Local\\Temp\\ipykernel_5620\\2618780480.py:11: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  clf.fit(os_train_X, os_train_y)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F-score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>logistic_regression_train_v1</th>\n",
       "      <td>0.767749</td>\n",
       "      <td>0.825442</td>\n",
       "      <td>0.893878</td>\n",
       "      <td>0.733736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_regression_test_v1</th>\n",
       "      <td>0.770607</td>\n",
       "      <td>0.832473</td>\n",
       "      <td>0.887441</td>\n",
       "      <td>0.735834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_regression_train_v2</th>\n",
       "      <td>0.777363</td>\n",
       "      <td>0.745064</td>\n",
       "      <td>0.854378</td>\n",
       "      <td>0.852135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_regression_test_v2</th>\n",
       "      <td>0.782388</td>\n",
       "      <td>0.753382</td>\n",
       "      <td>0.851896</td>\n",
       "      <td>0.844242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_train_v1</th>\n",
       "      <td>0.779465</td>\n",
       "      <td>0.746933</td>\n",
       "      <td>0.855958</td>\n",
       "      <td>0.854097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_test_v1</th>\n",
       "      <td>0.782691</td>\n",
       "      <td>0.753878</td>\n",
       "      <td>0.852686</td>\n",
       "      <td>0.842903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_train_v2</th>\n",
       "      <td>0.987022</td>\n",
       "      <td>0.979278</td>\n",
       "      <td>0.993022</td>\n",
       "      <td>0.995178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_test_v2</th>\n",
       "      <td>0.878598</td>\n",
       "      <td>0.869188</td>\n",
       "      <td>0.928910</td>\n",
       "      <td>0.888943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_train_v1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_test_v1</th>\n",
       "      <td>0.915290</td>\n",
       "      <td>0.935881</td>\n",
       "      <td>0.953791</td>\n",
       "      <td>0.897614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_train_v2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_train_v2</th>\n",
       "      <td>0.925596</td>\n",
       "      <td>0.923319</td>\n",
       "      <td>0.957346</td>\n",
       "      <td>0.927916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               F-score  Precision  Accuracy    Recall\n",
       "logistic_regression_train_v1  0.767749   0.825442  0.893878  0.733736\n",
       "logistic_regression_test_v1   0.770607   0.832473  0.887441  0.735834\n",
       "logistic_regression_train_v2  0.777363   0.745064  0.854378  0.852135\n",
       "logistic_regression_test_v2   0.782388   0.753382  0.851896  0.844242\n",
       "mlp_train_v1                  0.779465   0.746933  0.855958  0.854097\n",
       "mlp_test_v1                   0.782691   0.753878  0.852686  0.842903\n",
       "mlp_train_v2                  0.987022   0.979278  0.993022  0.995178\n",
       "mlp_test_v2                   0.878598   0.869188  0.928910  0.888943\n",
       "rf_train_v1                   1.000000   1.000000  1.000000  1.000000\n",
       "rf_test_v1                    0.915290   0.935881  0.953791  0.897614\n",
       "rf_train_v2                   1.000000   1.000000  1.000000  1.000000\n",
       "rf_train_v2                   0.925596   0.923319  0.957346  0.927916"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os = SMOTE(random_state=RANDOM_STATE)\n",
    "columns = X_train.columns\n",
    "os_train_X, os_train_y = os.fit_resample(X_train, y_train)\n",
    "os_train_X = pd.DataFrame(data=os_train_X, columns=columns )\n",
    "os_train_y = pd.DataFrame(data=os_train_y, columns=[TARGET_COL_NAME])\n",
    "\n",
    "print(\"Oversampled data length: \",len(os_train_y))\n",
    "print(\"Attrited customers proportion: \",len(os_train_y[os_train_y['Attrition_Flag']=='Attrited Customer'])/len(os_train_X))\n",
    "\n",
    "clf=RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "clf.fit(os_train_X, os_train_y)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "rf_scores_test = evaluate(y_test, y_pred, 'rf_test_v2')\n",
    "\n",
    "results_df = pd.concat([results_df,rf_scores_test], ignore_index=False, sort=False)\n",
    "results_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4c0530",
   "metadata": {},
   "source": [
    "##### Random forest v3: fitting the model (with oversampling and  hyperparameter tuning)\n",
    "Our classifier already produces feasible results, however there are some hyperparameters that can be used to train the model.  We haven't taken advantage of those and used the default values.\n",
    "We are going to use cross-validation implementation - GridSearchCV, which tries all possible combinations of provided hyperparameters values namly:\n",
    "- max_depth - The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
    "- max_features - The number of features to consider when looking for the best split\n",
    "- min_samples_leaf - The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n",
    "- min_samples_split - The minimum number of samples required to split an internal node.\n",
    "- n_estimators - The number of trees in the forest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f3646a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e9a9e1",
   "metadata": {},
   "source": [
    "Commented due to long execution time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1220e56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use the grid search to find the best hyperparameters\n",
    "# # First create the base model to tune\n",
    "# rf = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "# rf_gs = GridSearchCV(estimator = rf, param_grid = param_grid,\n",
    "#                           cv = 5, n_jobs = -1, verbose = 2)\n",
    "# rf_gs.fit(os_train_X, os_train_y)\n",
    "#\n",
    "# rf_gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7379314c",
   "metadata": {},
   "source": [
    "The Grid Search cross validation is a very expensive method, therefore we are going to stick with RandomSearchCV which only tries some combinations of hyperparameters.\n",
    "This way we won't test all the combinations, so we might not be able to find the best one, however the execution time will be shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e8814eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 15, stop = 250, num = 10)]\n",
    "n_estimators.append(100)\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 50, num = 5)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "ccp_alpha = [0.0001, 0]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap,\n",
    "               'ccp_alpha': ccp_alpha}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8ae339d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\romib\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:926: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self.best_estimator_.fit(X, y, **fit_params)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=RandomForestClassifier(random_state=5),\n",
       "                   n_iter=200, n_jobs=-1,\n",
       "                   param_distributions={'bootstrap': [True, False],\n",
       "                                        'ccp_alpha': [0.0001, 0],\n",
       "                                        'max_depth': [10, 20, 30, 40, 50, None],\n",
       "                                        'max_features': ['auto', 'sqrt'],\n",
       "                                        'min_samples_leaf': [1, 2, 4],\n",
       "                                        'min_samples_split': [2, 5, 10],\n",
       "                                        'n_estimators': [15, 41, 67, 93, 119,\n",
       "                                                         145, 171, 197, 223,\n",
       "                                                         250, 100]},\n",
       "                   random_state=5, scoring='f1_macro', verbose=2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state=RANDOM_STATE)\n",
    "rf_rs = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, scoring='f1_macro', n_iter = 200, cv = 5, verbose=2, random_state=RANDOM_STATE, n_jobs = -1)\n",
    "rf_rs.fit(os_train_X, os_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6979e317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F-score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>logistic_regression_train_v1</th>\n",
       "      <td>0.767749</td>\n",
       "      <td>0.825442</td>\n",
       "      <td>0.893878</td>\n",
       "      <td>0.733736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_regression_test_v1</th>\n",
       "      <td>0.770607</td>\n",
       "      <td>0.832473</td>\n",
       "      <td>0.887441</td>\n",
       "      <td>0.735834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_regression_train_v2</th>\n",
       "      <td>0.777363</td>\n",
       "      <td>0.745064</td>\n",
       "      <td>0.854378</td>\n",
       "      <td>0.852135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_regression_test_v2</th>\n",
       "      <td>0.782388</td>\n",
       "      <td>0.753382</td>\n",
       "      <td>0.851896</td>\n",
       "      <td>0.844242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_train_v1</th>\n",
       "      <td>0.779465</td>\n",
       "      <td>0.746933</td>\n",
       "      <td>0.855958</td>\n",
       "      <td>0.854097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_test_v1</th>\n",
       "      <td>0.782691</td>\n",
       "      <td>0.753878</td>\n",
       "      <td>0.852686</td>\n",
       "      <td>0.842903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_train_v2</th>\n",
       "      <td>0.987022</td>\n",
       "      <td>0.979278</td>\n",
       "      <td>0.993022</td>\n",
       "      <td>0.995178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_test_v2</th>\n",
       "      <td>0.878598</td>\n",
       "      <td>0.869188</td>\n",
       "      <td>0.928910</td>\n",
       "      <td>0.888943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_train_v1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_test_v1</th>\n",
       "      <td>0.915290</td>\n",
       "      <td>0.935881</td>\n",
       "      <td>0.953791</td>\n",
       "      <td>0.897614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_train_v2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_train_v2</th>\n",
       "      <td>0.925596</td>\n",
       "      <td>0.923319</td>\n",
       "      <td>0.957346</td>\n",
       "      <td>0.927916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_train_v3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_test_v3</th>\n",
       "      <td>0.925272</td>\n",
       "      <td>0.928846</td>\n",
       "      <td>0.957741</td>\n",
       "      <td>0.921797</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               F-score  Precision  Accuracy    Recall\n",
       "logistic_regression_train_v1  0.767749   0.825442  0.893878  0.733736\n",
       "logistic_regression_test_v1   0.770607   0.832473  0.887441  0.735834\n",
       "logistic_regression_train_v2  0.777363   0.745064  0.854378  0.852135\n",
       "logistic_regression_test_v2   0.782388   0.753382  0.851896  0.844242\n",
       "mlp_train_v1                  0.779465   0.746933  0.855958  0.854097\n",
       "mlp_test_v1                   0.782691   0.753878  0.852686  0.842903\n",
       "mlp_train_v2                  0.987022   0.979278  0.993022  0.995178\n",
       "mlp_test_v2                   0.878598   0.869188  0.928910  0.888943\n",
       "rf_train_v1                   1.000000   1.000000  1.000000  1.000000\n",
       "rf_test_v1                    0.915290   0.935881  0.953791  0.897614\n",
       "rf_train_v2                   1.000000   1.000000  1.000000  1.000000\n",
       "rf_train_v2                   0.925596   0.923319  0.957346  0.927916\n",
       "rf_train_v3                   1.000000   1.000000  1.000000  1.000000\n",
       "rf_test_v3                    0.925272   0.928846  0.957741  0.921797"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rf_rs.predict(X_test)\n",
    "rf_scores_test = evaluate(y_test, y_pred, 'rf_test_v3')\n",
    "\n",
    "results_df = pd.concat([results_df,rf_scores_test], ignore_index=False, sort=False)\n",
    "results_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3d5c30",
   "metadata": {},
   "source": [
    "The Random Forest classifier has shown superior performance compared to Logistic Regression and Multi-Layer Perceptron on the provided dataset. Random Forest excels in handling complex and high-dimensional data by constructing an ensemble of decision trees and aggregating their predictions. It effectively captures non-linear relationships, interactions, and feature importance, which can be crucial in achieving accurate predictions. Additionally, Random Forest mitigates overfitting by using bootstrap sampling and feature randomization. Also it is commonly used method when we deal with unbalanced data, which is our case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384c6136",
   "metadata": {},
   "source": [
    "### Neural Networks with PyTorch\n",
    "\n",
    "We made the decision to implement our own neural network architecture using PyTorch instead of relying on the multi-layer perceptron (MLP) from the scikit-learn library for several reasons. Firstly, PyTorch provides a more flexible and intuitive framework for building and training neural networks. With PyTorch, we have fine-grained control over the network architecture, allowing us to customize each layer and easily experiment with various activation functions, optimizers, and loss functions. This flexibility is essential when working with complex datasets or tackling challenging tasks. By implementing our own architecture with PyTorch, we can harness the full potential of deep learning techniques and adapt them to our specific needs, ultimately leading to improved performance and more accurate models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a916f750",
   "metadata": {},
   "source": [
    "The biggest challenge we encountered with our custom architecture was overfitting, where the model performed exceptionally well on the training data but struggled to generalize to unseen examples. To address this issue, we employed multiple regularization methods to improve the model's generalization ability. \n",
    "- Firstly, we applied dropout, which randomly deactivates a certain percentage of neurons during training, forcing the network to rely on different sets of features and preventing over-reliance on specific weights. This technique helps reduce overfitting by promoting model robustness.\n",
    "- Additionally, we incorporated batch normalization, which normalizes the inputs within each mini-batch during training. This technique helps stabilize and standardize the activations across different layers, making the network less sensitive to parameter initialization and reducing the risk of overfitting.\n",
    "- To further combat overfitting, we utilized an optimizer with L2 regularization, such as weight decay, which adds a penalty term to the loss function that discourages large weight values. This regularization method helps prevent the model from becoming overly complex by reducing the magnitude of the weights and encourages the network to focus on more important features, leading to improved generalization.\n",
    "- Lastly, we applied L2 regularization directly to the network's weights themselves. This technique adds a penalty term to the loss function based on the sum of the squared weights, effectively shrinking the weights towards zero and preventing any individual weight from dominating the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "75e8099b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20, Train Loss: 0.3169825724526765, Test Loss: 0.1363535374403\n",
      "Epoch 2/20, Train Loss: 0.12779949812554778, Test Loss: 0.12263014912605286\n",
      "Epoch 3/20, Train Loss: 0.10871562125774416, Test Loss: 0.11508418619632721\n",
      "Epoch 4/20, Train Loss: 0.09680716013960508, Test Loss: 0.11114788800477982\n",
      "Epoch 5/20, Train Loss: 0.0886753428166617, Test Loss: 0.11070992797613144\n",
      "Epoch 6/20, Train Loss: 0.08273211947591658, Test Loss: 0.1437973529100418\n",
      "Epoch 7/20, Train Loss: 0.07832321753116937, Test Loss: 0.10611317306756973\n",
      "Epoch 8/20, Train Loss: 0.07319263218297652, Test Loss: 0.09247367084026337\n",
      "Epoch 9/20, Train Loss: 0.06907344037558719, Test Loss: 0.09629832953214645\n",
      "Epoch 10/20, Train Loss: 0.06444595380686806, Test Loss: 0.09943680465221405\n",
      "Epoch 11/20, Train Loss: 0.06472429833249849, Test Loss: 0.10425776988267899\n",
      "Epoch 12/20, Train Loss: 0.06179588451413793, Test Loss: 0.10585980117321014\n",
      "Epoch 13/20, Train Loss: 0.060518190774282464, Test Loss: 0.09504636377096176\n",
      "Epoch 14/20, Train Loss: 0.057121776972782756, Test Loss: 0.09230352193117142\n",
      "Epoch 15/20, Train Loss: 0.05709211242098424, Test Loss: 0.09093483537435532\n",
      "Epoch 16/20, Train Loss: 0.056290941414127195, Test Loss: 0.09107007831335068\n",
      "Epoch 17/20, Train Loss: 0.05599533955913868, Test Loss: 0.09272931516170502\n",
      "Epoch 18/20, Train Loss: 0.05480846819665043, Test Loss: 0.08773737400770187\n",
      "Epoch 19/20, Train Loss: 0.053741635623237326, Test Loss: 0.08659052103757858\n",
      "Epoch 20/20, Train Loss: 0.05286974322602274, Test Loss: 0.0895884707570076\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_tensor = torch.Tensor(X_train_balanced)\n",
    "y_train_tensor = torch.Tensor(y_train_balanced)\n",
    "X_test_tensor = torch.Tensor(X_test_scaled)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "\n",
    "# Define our neural network architecture\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.dropout1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.dropout2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "input_size = X_train_tensor.shape[1]\n",
    "model = NeuralNet(input_size)\n",
    "\n",
    "# We defined the loss function and optimizer with L2 regularization\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', patience=3, verbose=True)\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Start training\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for batch_inputs, batch_labels in train_data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(batch_inputs)\n",
    "        loss = criterion(outputs, batch_labels.view(-1, 1))\n",
    "        \n",
    "        # L2 regularization\n",
    "        l2_lambda = 0.001\n",
    "        l2_regularization = torch.tensor(0.)\n",
    "        for param in model.parameters():\n",
    "            l2_regularization += torch.norm(param, 2)\n",
    "        loss += l2_lambda * l2_regularization\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * batch_inputs.size(0)\n",
    "    \n",
    "    # Update the learning rate as we made its value dynamic\n",
    "    scheduler.step(running_loss)\n",
    "    \n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    train_losses.append(epoch_loss)\n",
    "    \n",
    "    # Evaluate the model on the test data\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        test_outputs = model(X_test_tensor)\n",
    "        test_loss = criterion(test_outputs, y_test.view(-1, 1))\n",
    "        test_losses.append(test_loss.item())\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss}, Test Loss: {test_loss.item()}\")\n",
    "\n",
    "\n",
    "# Vizualization\n",
    "plt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, num_epochs+1), test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Get predictions on unseed data\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    predicted_probs = model(X_test_tensor)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "predicted_labels = (predicted_probs >= 0.5).float().squeeze().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af51756",
   "metadata": {},
   "source": [
    "From this graph we can clearly see if the model will be overfitting, test loss would be increasing and train loss decreasing. If we add more epochs we would be still able to observe this so in that case there is possibility to add more regularization or make architecture more simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7fe5aab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F-score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>logistic_regression_train_v1</th>\n",
       "      <td>0.767749</td>\n",
       "      <td>0.825442</td>\n",
       "      <td>0.893878</td>\n",
       "      <td>0.733736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_regression_test_v1</th>\n",
       "      <td>0.770607</td>\n",
       "      <td>0.832473</td>\n",
       "      <td>0.887441</td>\n",
       "      <td>0.735834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_regression_train_v2</th>\n",
       "      <td>0.777363</td>\n",
       "      <td>0.745064</td>\n",
       "      <td>0.854378</td>\n",
       "      <td>0.852135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>logistic_regression_test_v2</th>\n",
       "      <td>0.782388</td>\n",
       "      <td>0.753382</td>\n",
       "      <td>0.851896</td>\n",
       "      <td>0.844242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_train_v1</th>\n",
       "      <td>0.779465</td>\n",
       "      <td>0.746933</td>\n",
       "      <td>0.855958</td>\n",
       "      <td>0.854097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_test_v1</th>\n",
       "      <td>0.782691</td>\n",
       "      <td>0.753878</td>\n",
       "      <td>0.852686</td>\n",
       "      <td>0.842903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_train_v2</th>\n",
       "      <td>0.987022</td>\n",
       "      <td>0.979278</td>\n",
       "      <td>0.993022</td>\n",
       "      <td>0.995178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp_test_v2</th>\n",
       "      <td>0.878598</td>\n",
       "      <td>0.869188</td>\n",
       "      <td>0.928910</td>\n",
       "      <td>0.888943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_train_v1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_test_v1</th>\n",
       "      <td>0.915290</td>\n",
       "      <td>0.935881</td>\n",
       "      <td>0.953791</td>\n",
       "      <td>0.897614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_train_v2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_train_v2</th>\n",
       "      <td>0.925596</td>\n",
       "      <td>0.923319</td>\n",
       "      <td>0.957346</td>\n",
       "      <td>0.927916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_train_v3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf_test_v3</th>\n",
       "      <td>0.925272</td>\n",
       "      <td>0.928846</td>\n",
       "      <td>0.957741</td>\n",
       "      <td>0.921797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Custom NN</th>\n",
       "      <td>0.946764</td>\n",
       "      <td>0.935302</td>\n",
       "      <td>0.968799</td>\n",
       "      <td>0.959357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>custom_NN_test</th>\n",
       "      <td>0.946764</td>\n",
       "      <td>0.935302</td>\n",
       "      <td>0.968799</td>\n",
       "      <td>0.959357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               F-score  Precision  Accuracy    Recall\n",
       "logistic_regression_train_v1  0.767749   0.825442  0.893878  0.733736\n",
       "logistic_regression_test_v1   0.770607   0.832473  0.887441  0.735834\n",
       "logistic_regression_train_v2  0.777363   0.745064  0.854378  0.852135\n",
       "logistic_regression_test_v2   0.782388   0.753382  0.851896  0.844242\n",
       "mlp_train_v1                  0.779465   0.746933  0.855958  0.854097\n",
       "mlp_test_v1                   0.782691   0.753878  0.852686  0.842903\n",
       "mlp_train_v2                  0.987022   0.979278  0.993022  0.995178\n",
       "mlp_test_v2                   0.878598   0.869188  0.928910  0.888943\n",
       "rf_train_v1                   1.000000   1.000000  1.000000  1.000000\n",
       "rf_test_v1                    0.915290   0.935881  0.953791  0.897614\n",
       "rf_train_v2                   1.000000   1.000000  1.000000  1.000000\n",
       "rf_train_v2                   0.925596   0.923319  0.957346  0.927916\n",
       "rf_train_v3                   1.000000   1.000000  1.000000  1.000000\n",
       "rf_test_v3                    0.925272   0.928846  0.957741  0.921797\n",
       "Custom NN                     0.946764   0.935302  0.968799  0.959357\n",
       "custom_NN_test                0.946764   0.935302  0.968799  0.959357"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_np = y_test.numpy()\n",
    "\n",
    "rf_scores_test = evaluate(y_test_np, predicted_labels, 'custom_NN_test')\n",
    "results_df = pd.concat([results_df,rf_scores_test], ignore_index=False, sort=False)\n",
    "results_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8468c25e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313edbc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795e8ad1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
